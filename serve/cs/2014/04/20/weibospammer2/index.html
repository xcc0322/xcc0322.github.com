
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">

<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">
<head>
   <meta http-equiv="content-type" content="text/html; charset=utf-8" />
   <title>WeiboSpammer-Chapter2 WeiboCrawler using Scrapy &larr; </title>
   <meta name="author" content="Chengcheng" />

   <link rel="start" href="/" />

	
	
	
  	<link rel="alternate" type="application/atom+xml" href="http://feeds.feedburner.com/feedname" title="RSS feed" />
	
	

   <!-- syntax highlighting CSS -->
   <link rel="stylesheet" href="/assets/themes/mark-reid/css/syntax.css" type="text/css" />

   <!-- Homepage CSS -->
   <link rel="stylesheet" href="/assets/themes/mark-reid/css/screen.css" type="text/css" />

</head>
<body id="">
<div id="site">
  
  <div id="header">
    <h1>
    	<a href="/" title="Life">Life</a>
    	<span class="byline">&larr; <a href="/">Chengcheng</a></span>
    </h1>
    <ul class="nav">
      <li><a class="home" href="/">Home</a></li>
      <li><a  href="/archive.html">Archive</a></li>
      <!--<li><a  href="/pages.html">Pages</a></li>-->
      <li><a  href="/categories.html">Categories</a></li>
      <li><a  href="/pages/contacts.html">Contact</a></li>
      <li><a  href="/pages/links.html">Links</a></li>
    </ul>
  </div>

  
<div id="page">
	
  <h1 class="emphnext">WeiboSpammer-Chapter2 WeiboCrawler using Scrapy</h1>
  <ul class="tag_box inline">
  
  


  
     
    	<li><a href="/tags.html#Weibo Spammer-ref">Weibo Spammer <span>3</span></a></li>
     
    	<li><a href="/tags.html#CS course-ref">CS course <span>3</span></a></li>
    
  



  </ul>

  <p>After a bite of Scrapy, we are going to make it work with our target sites, weibo in this case.</p>

<p>According to the need of my project, I design a crawler with following jobs:</p>

<ol>
<li>
<p>Scratch basic info(numbers of followings and fans, quality of most recent posts) as a signal judging the possibility of anomaly.</p>
</li>

<li>
<p>Move to the following and fans pages to crawl the uids of the <em>followed users(followings)</em> and <em>fans</em>. Store the connections to build the link graph.</p>
</li>

<li>
<p>Crawl and add the profile pages to request list of the users we met in the second step. Repeat step1 and step2.</p>
</li>
</ol>

<h4 id='log_in'>Log in</h4>

<p>Tricky is to log in and acquire the first page. I referred to the work <a href='http://qinxuye.me/article/simulate-weibo-login-in-python/'>here</a>.</p>

<p>The machinism is a little complicated. We would have to observe the rules little by little with Firebug-like tools. Oberserve parameters in our posts and the content returned by weibo server.</p>

<p>After following the steps by the link above, I finally make it arrive at the first user profile page of mine. Making sure that this works, I start to build the spider.</p>

<h4 id='customize_scrapy'>Customize Scrapy</h4>

<p>The main tasks here are to make our own <strong>Spider, Item and ItemPipeline</strong> classes, as well as to modify some settings.</p>

<h6 id='spider'>Spider</h6>

<p>In Spider class, I build the log-in, parsing and extracting data, and crawling further links procedures. Make every procedure a seperate class and assembly them in WeiboSpider class.</p>

<p>The concepts are simple. Here are some key points.</p>

<p>Remember the graph in Chapter 1. The output of Spider class should be Requests and Items to be dispatched to Downloader and ItemPipeline respectively. We either set some <em>start_urls</em> list or define the <em>start_request</em> function to generate the first pages. And make every function deal with the response instance and return a list of Items and Requests like this one:</p>

<pre><code>return [
    Request(
        following_url,
        headers = self.headers,
        callback = self.parse_following_page
    ),Request(
        fans_url,
        headers = self.headers,
        callback = self.parse_fans_page
    ), item
]</code></pre>

<p>Scrapy use an asynchronization machenism to increase the effiency. Return the Request instance with call_back function and the Scheduler would schedule automatically.</p>

<p>Extracting links and data with certain pattern in HTML requires using XPath of Selector class.</p>

<pre><code>from scrapy.selector import Selector

self.sel = Selector(response)

# extract all user ids on the page
uid_list = self.sel.xpath(&#39;//a/@href&#39;).re(r&#39;.*uid=(\d+).*&#39;)

# extract urls of friends&#39; profile pages
urls = self.sel.xpath(&#39;//table/tr/td[2]/a[1]/@href&#39;).extract()

# extract next page url (下页 in Chinese)
next_page = self.sel.xpath(u&#39;//a[text()=\&#39;下页\&#39;]/@href&#39;).extract()

# extract number of commnets (评论 in Chinese)
comments = self.sel.xpath(&#39;//a/text()&#39;).re(ur&#39;^评论\[(\d+)\]&#39;)</code></pre>

<h6 id='run'>Run</h6>

<p>The Item class definition requires almost no tricks. After building the Spider and Item class as well as customize the settings file, we have made a working crawler which could crawl and extract the target item as wanted.</p>

<p>Turn on the logging in Spider and run the crawler, we will see the details in LOG_FILE</p>

<pre><code>log.start(logfile=&#39;LOG_FILE&#39;)
&gt;scrapy crawl weibo</code></pre>

<p>We could direct the crawler to output items in a json format to local file.</p>

<pre><code>&gt;scrapy crawl weibo -o items.json -t json</code></pre>

<p>Congratulations! If the .json file fulfills your data requirement, then your work has finished.</p>

<h6 id='itempipeline'>ItemPipeline</h6>

<p>Itempipeline is for filters and further export operations on result Items. Complicated export operations may also be defined in Item Exporters.</p>

<p>I stored my data in MySql database, so I make a connection with my db using twisted api and deal with tables here. Before the crawling, set up the database and create the tables.</p>

<pre><code>from twisted.enterprise import adbapi

self.dbpool = adbapi.ConnectionPool(&#39;MySQLdb&#39;, **dbargs)
d = self.dbpool.runInteraction(self._do_upsert, item, spider)

def _do_upsert(self, conn, item, spider):</code></pre>

<p>Refer dirbot educational example for further details.</p>

<p>Now I have completed my crawler and store the results in Mysql database:)</p>

<h4 id='problems__solutions'>Problems &amp; Solutions</h4>

<h6 id='request_priority'>Request Priority</h6>

<p>In Spider class, my extracted links include the <strong>friends profile pages</strong> and <strong>next pages</strong>. The next pages are due to the limit of total friends displaying in one page. I want my crawler to follow a bread-first method. Here the priority feature of Request Objects solves my problem. Just set <strong>next pages</strong> a higher priority than <strong>friends profile pages</strong>!</p>

<h6 id='pause_and_resumefailed'>Pause and Resume(failed)</h6>

<p>Why I need to pause and resume the crawer?</p>

<ul>
<li>
<p>I want a larger dataset, as this is a link analysis on a social graph. My laptop usage are not sure to be insistant and our network is unstable.</p>
</li>

<li>
<p>weibo has some prohibition that causes a frequent stop.</p>
</li>
</ul>

<p>The official document provide <a href='http://doc.scrapy.org/en/latest/topics/jobs.html?highlight=pause'>a way</a> to do this by direct a JOBDIR to store the status of unfinished Requests and Request Filters. This requires you to keep serializable characteristic.</p>

<pre><code>&gt;scrapy crawl weibo -s JOBDIR=cache/persistence</code></pre>

<p>However, I failed to work with it. Maybe I have some problems with the serializable requirement. As I later found the data volume is passable for my project, I gave up this feature.</p>

<h4 id='bug_shooting__tips'>Bug Shooting &amp; Tips</h4>

<ol>
<li>
<p>Helpful references:</p>

<ul>
<li><a href='http://deerchao.net/tutorials/regex/regex.htm'>regex</a></li>

<li><a href='http://www.w3schools.com/XPath/'>XPath</a></li>
</ul>
</li>

<li>
<p>If you want to use Mysql, install mysql_python with easy_install first.</p>
</li>
</ol>

  <address class="signature">
    <a class="author" href="/">Chengcheng</a> 
    <span class="date">20 April 2014</span>
    <span class="location"></span>
  </address>
  
  <div class="prev-next">
  
    <a href="/University/2014/04/23/wonderfullecture" class="next" title="王德峰 中国智慧与当代生活 讲座笔记">Next Post: 王德峰 中国智慧与当代生活 讲座笔记&rarr;</a>
  
  
    <a href="/cs/2014/04/20/weibospammer1" class="prev" title="WeiboSpammer-Chapter1 Scrapy">&larr; Earlier Post</a>
  
  </div>
  
</div><!-- End Page -->




  <div id="disqus_thread"></div>
<script type="text/javascript">
    var disqus_developer = 1;
    var disqus_shortname = 'fdxcc'; // required: replace example with your forum shortname
    
    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = 'http://' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">blog comments powered by <span class="logo-disqus">Disqus</span></a>






  
  <div id="footer">
  	<address>
  		<span class="copyright">
			Content by <a href="mailto:xcc0322@gmail.com">Chengcheng</a>.
			<br/>
		       	Theme Design by Mark Reid.
			<br/>
  			<a rel="licence" href="http://creativecommons.org/licenses/by-nc-sa/3.0/">Some rights reserved</a>			
  		</span>
  		<span class="engine">
  			Powered by <a href="http://github.com/mojombo/jekyll/" title="A static, minimalist CMS">Jekyll</a>
  		</span>
  	</address>
  </div>
  
</div>

<!--[if IE 6]>
<script type="text/javascript"> 
	/*Load jQuery if not already loaded*/ if(typeof jQuery == 'undefined'){ document.write("<script type=\"text/javascript\"   src=\"http://ajax.googleapis.com/ajax/libs/jquery/1.3.2/jquery.min.js\"></"+"script>"); var __noconflict = true; } 
	var IE6UPDATE_OPTIONS = {
		icons_path: "http://static.ie6update.com/hosted/ie6update/images/"
	}
</script>
<script type="text/javascript" src="http://static.ie6update.com/hosted/ie6update/ie6update.js"></script>
<![endif]-->

  
</body>
</html>

