<?xml version="1.0" encoding="UTF-8" ?>
<rss version="2.0">
<channel>
        <title>Life</title>
        <description>Life - Chengcheng</description>
        <link>http://xcc0322.github.io</link>
        <link>http://xcc0322.github.io</link>
        <lastBuildDate>2014-11-14T21:05:32+08:00</lastBuildDate>
        <pubDate>2014-11-14T21:05:32+08:00</pubDate>
        <ttl>1800</ttl>


        <item>
                <title>六月意识流</title>
                <description>&lt;p&gt;&lt;img alt='photo' src='http://i10.topit.me/l105/10105582420f788e7f.jpg' style='width:100%' /&gt;&lt;/p&gt;

&lt;p&gt;进入期末了。今天考完这学期第一门考试。&lt;/p&gt;

&lt;p&gt;写点意识流给此刻的我留个快照。&lt;/p&gt;

&lt;h2 id='oo'&gt;OO&lt;/h2&gt;

&lt;p&gt;从今天倒叙。&lt;/p&gt;

&lt;p&gt;今天考完的这门课名叫面向对象技术，是这学期比较有收获的一门课。张天戈老师有一种老工程师的靠谱气质，和老爸有某种神似。于是上课的时候有一种额外的亲切感，听课比较认真。课本&lt;a href='http://www.amazon.cn/gp/product/B00116WMSU/ref=pd_lpo_k2_dp_sr_1?pf_rd_p=60080992&amp;amp;pf_rd_s=lpo-top-stripe&amp;amp;pf_rd_t=201&amp;amp;pf_rd_i=B005EE2ISE&amp;amp;pf_rd_m=A1AJ19PSB66TGU&amp;amp;pf_rd_r=0SJXMFGV8RTBZ42MM274'&gt;UML和模式应用&lt;/a&gt;，由于英文版价格过高，只好委屈于翻译极烂的中文版（暂时是我见过课本翻译中最烂的）。为了给读者更生动的敏捷开发体会，全书的结构也是按迭代式由浅入深进行的。由于译本太烂，我也是断断续续过了三遍才把握了大部分内容。期末突击的同学抱怨这书不着调，大概和我第一次认真看书的体验差不多。总结来看，书的内容还是非常不错的，结合案例、图文并茂、言传身教，非常生动地把整个运用了oo的up开发过程展现出来，每一条经验都是实打实的干货。&lt;/p&gt;

&lt;p&gt;上课期间，老师留下一些参考阅读材料，都像老师本人一样，是非常经典的干货&lt;a href='http://www.amazon.cn/%E5%88%86%E6%9E%90%E6%A8%A1%E5%BC%8F-%E5%8F%AF%E5%A4%8D%E7%94%A8%E7%9A%84%E5%AF%B9%E8%B1%A1%E6%A8%A1%E5%9E%8B-%E7%A6%8F%E5%8B%92/dp/B004BA21SY/ref=sr_1_1?s=books&amp;amp;ie=UTF8&amp;amp;qid=1402311122&amp;amp;sr=1-1&amp;amp;keywords=%E5%88%86%E6%9E%90%E6%A8%A1%E5%BC%8F'&gt;《分析模式》&lt;/a&gt;,&lt;a href='http://www.amazon.cn/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%9B%E4%B9%A6-%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E5%8F%AF%E5%A4%8D%E7%94%A8%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E8%BD%AF%E4%BB%B6%E7%9A%84%E5%9F%BA%E7%A1%80-Erich-Gamma/dp/B001130JN8/ref=sr_1_1?s=books&amp;amp;ie=UTF8&amp;amp;qid=1402311177&amp;amp;sr=1-1&amp;amp;keywords=%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F'&gt;《设计模式》&lt;/a&gt;,&lt;a href='http://www.amazon.cn/%E4%BB%A3%E7%A0%81%E5%A4%A7%E5%85%A8-%E5%8F%B2%E8%92%82%E5%A4%AB%E2%80%A2%E8%BF%88%E5%85%8B%E5%BA%B7%E5%A5%88%E5%B0%94/dp/B0061XKRXA/ref=sr_1_1?s=books&amp;amp;ie=UTF8&amp;amp;qid=1402311217&amp;amp;sr=1-1&amp;amp;keywords=%E4%BB%A3%E7%A0%81%E5%A4%A7%E5%85%A8'&gt;《代码大全》&lt;/a&gt;。不过这些书需要一定的经验，略显厚重。这次准备开卷考试过程里，得以把当年图书全场五折抢购的英文版《head first设计模式》仔细阅读了一遍。也是很赞的书！&lt;/p&gt;

&lt;p&gt;非常庆幸自己在这学期的软件工程课和oo课，有过去年实习+软件实践课两次比较完整采用Scrum的开发经历。只有经历过的人，才能感受到那每一条经验的总结都是实打实的干货。后验地戴上“工程”的眼镜审视自己的开发过程是一种很奇妙的感觉。顿觉今年实习一定有质的飞跃。&lt;/p&gt;

&lt;h2 id='id11'&gt;拍照&lt;/h2&gt;

&lt;p&gt;再说昨天。&lt;/p&gt;

&lt;p&gt;考试前一天周日。我和501中分美少女们出门拍写真照片。目测娄神会看到这篇日志，在此要再次感谢娄神的辛勤劳动！摄影技术好的人有千千万，但找到一个值得信赖的、愿意帮我们拍写真照片又帮ps的热情靠谱的好人就很难得了！总觉得拍写真照片是一件比较尴尬的事，要是自己一人的话，我肯定不好意思麻烦摄影师同学。不过有501的美少女们组团出行，这次活动还是圆满成功啦~&lt;/p&gt;

&lt;p&gt;经验告诉我，拍写真照片当场比较尴尬，但认真地定格这一瞬间，和亲爱的人留下一些美丽的资料还是很好的事情。说来也比较巧合，最后选择出境的连衣裙正好是整三年前高考最后一天下午我参加毕业典礼的那一件。三年前的夏天，我订了一个超大的双层补丁的蛋糕，在二中的学生广场上等待刚考完最后一门基本能力测试的同班同学。和美少女们躺在张江的草地上仰望天空。那一刻，眼里只有四片翠绿的树叶和蓝天背景。6.8下午那片草地和美少女们定格的那个瞬间要留在我心里一段时间了。我想，我的青春就这样又走过了三年唉。&lt;/p&gt;

&lt;p&gt;今天是高中毕业整三年，今天的大山东又有五十万的少年结束了高考。上午看到大四学长姐在草地拍毕业照了，仿佛穿过时间的隧道一下子看到了一年后的自己。今年春季减肥计划失败，暑假面对dream食堂估计也瘦不下来，只有期待大四再战了。看到中分美少女们纷纷越来越瘦，越来越美心中既欢喜又忧伤TT。明年毕业留念之前一定要变美丽！&lt;/p&gt;

&lt;h2 id='id12'&gt;退*&lt;/h2&gt;

&lt;p&gt;昨天还有一件事印象比较深，晚饭回寝本想认真复习，被捉去参加组织大会。听闻学姐提交退出组织的申请书一事。&lt;/p&gt;

&lt;p&gt;不得不给学姐的执行力点个赞。也庆幸各种环境为学姐提供了一条得以出国远离体制内，不再受某些束缚，可以敢想敢做，退出当下这个捉急的组织。&lt;/p&gt;

&lt;p&gt;想想大一对组织的看法其实和现在的学姐是差不多的，但是充满着各种怀疑的我一直懒得仔细思考这个问题。不过大二在台湾的时候在政治矛盾很多的地方得以思考了许多关于自己，关于周围环境的政治问题，终于考虑清楚了自己继续留在组织的决定和动机。昨天这件事又让我回忆起那个爱思考的青年。顺便给那时的自己也点个赞。&lt;/p&gt;

&lt;p&gt;内容太敏感，考虑了一下还是不写出来了。&lt;/p&gt;

&lt;h2 id='id13'&gt;面基&lt;/h2&gt;

&lt;p&gt;端午节的两天里分别在本部和枫林会见了二中-复旦小伙伴们和大一501（我本部和张江的寝室竟然同号）的三个医学生小伙伴。两年过去，看到专业不同的小伙伴们都已在专业的岔路过后前行了一段距离。学化学的数学的世经的微电的医学的药学的小伙伴各自说着不同的专业名词，各自遇到的新鲜事，在嬉笑的吐槽里我又长知识了。比较反常的是，两次吃饭都是我拉大家出来的。令我感到欣慰的是，大家居然都到齐了。心里感到十分温暖。&lt;/p&gt;

&lt;h2 id='id14'&gt;六月&lt;/h2&gt;

&lt;p&gt;距离7月7日的实习，前方的任务还剩明天的英语论说文作文的考试，编译第三阶段的pj一个，数据挖掘的期末报告一份，软件工程、编译、体系结构的期末考试各一门。 最后一门考试在6.30结束后，7.1-7.6回青岛爽一周，接着就是期待已久的实习了。&lt;/p&gt;

&lt;h2 id='id15'&gt;一个月以后的实习&lt;/h2&gt;

&lt;p&gt;上个月印象最深刻的事情要属host离职了。宣讲会回公司玩的那个周一收到host微信说自己要离职去创业公司了，瞬间感到十分郁闷。周四回去虽然见到了欢乐的小伙伴们，但是没能见到host。于是周六单独会见师父，开始还以为师父要拉我去创业公司实习（这样的话，以师父的个人魅力，我还真的要考虑考虑）。结果是师父在最后一次one one的晚饭中实在是不浪费最后一刻言传身教把自己的工作经历，跳槽的过程甚至感情经历都告诉我，还指导我短期今年夏天回去换组的话推荐去哪，可以找谁作host，可以哪些活动，可以利用什么资源，长期接下来职业发展去了美国可以如何如何，顺便还透露了一些平常听不到的小八卦。感到我有多信任我师父，师父就有多信任我T T去年碰到我这么喜欢的组，还有信任我的靠谱低调的师父多不容易！回來的地铁上差点又像去年离职一样激动得泪流满面了。&lt;/p&gt;

&lt;p&gt;接着的星期三又去蹭了师父的离职小龙虾。目测host心里十分难过，但还是坚持维持了全场欢乐的气氛。&lt;/p&gt;

&lt;p&gt;后来我把最后一次one one的谈话记录好好地写了写收藏了下来，接受了今年实习host离我而去的事实，下定决心，今年实习一定要出色地完成任务，延续去年巅峰的学习曲线，然后拿到return offer，然后多攒rp一举抽到h1b毕业就到mtv去。&lt;/p&gt;

&lt;p&gt;今年的实习虽然没能到mtv去，但留在欢乐熟悉又好吃的上海office我也很满意。想到一个月后又可以和靠谱的同事一起工作，吃到dream食堂就激动。值得一提的是，今年的实习生还没进去就认识了好多。同班同学2枚，高中同学1枚，外加新去的学妹x枚，和去年os lab的助教去做research intern1枚。&lt;/p&gt;

&lt;p&gt;毕业留在这间公司是大约三月份时做的这个决定。记得那天下午为终于结束纠结做出决定开心了很久，快乐地走在校园里情不自禁地要笑出来。所谓“不到抛硬币的那一刻，你才知道自己心里到底期待的是哪一面”。每一个毕业选择都是各有利弊，很开心能找到一个感性和理性都满意的决定。&lt;/p&gt;

&lt;h2 id='id16'&gt;还有什么想说的&lt;/h2&gt;

&lt;p&gt;我的成长就是不断在浪漫主义和现实主义，感性与理性之间寻找最舒服的调配比例。很开心在大学三年里看到那条变化的曲线趋于平稳了。&lt;/p&gt;

&lt;p&gt;最近我感到成长的迷茫在一点一点散去，“毕业”两个字渐渐不再那么不忍直视了。&lt;/p&gt;

&lt;p&gt;择偶标准？想了想觉得基本原则要靠谱，要聪明，要有品位，在此基础上追求上述那个调配比例与我越接近越好。要与我有缘分（与企鹅招聘不要运气不好的人同理）。我真心觉得我面对爱情的态度开放又端正，才不是什么高霸上女神什么的。&lt;/p&gt;

&lt;p&gt;最近（学生运动十五周年之际）GFW把某歌彻底屏蔽了。吐槽无力。&lt;/p&gt;

&lt;p&gt;赶不及似的，我和时间粗鲁地抢下这些记忆的掠影，混乱地放在这里。&lt;/p&gt;</description>
                <link>http://xcc0322.github.io/2014/06/09/june2014</link>
                <guid>http://xcc0322.github.io/2014/06/09/june2014</guid>
                <pubDate>2014-06-09T00:00:00+08:00</pubDate>
        </item>

        <item>
                <title>王德峰 中国智慧与当代生活 讲座笔记</title>
                <description>&lt;p&gt;4/22/2014 10:15:06 PM&lt;/p&gt;

&lt;p&gt;&lt;img alt='photo' src='http://f11.topit.me/o/201001/29/12646997605041.jpg' style='width:100%' /&gt;&lt;/p&gt;

&lt;p&gt;今晚张江的学生组织终于邀请到了王德峰，把之前开在三教的讲座请来。讲座内容很有趣，理解难度适中，很有启发性。加上讲者是我偶像，我听讲认真，收获很大。这篇日志以讲座记录为主，我的注解不多。&lt;/p&gt;

&lt;p&gt;王德峰教授是研究中国哲学的专家。这次讲座的内容与题目吻合，深入浅出地介绍了儒、道、佛三大中国文化智慧结晶的核心概念，重点部分都配合了丰富的例子增强理解。其中一大重点是对禅宗中“悟”的过程的讨论。这个概念极其抽象，但老师生动地讲述六祖慧能的故事，并配合恰到好处的解释实在精彩到位，让我这个没研究过佛学的门外汉感到一种与佛说前所未有的默契，有多个时刻在我的心里留下一种刹那领悟的感动，让我对“佛”的理解一下进步了很多。&lt;/p&gt;

&lt;p&gt;这个讲座也激发了我对中国哲学的兴趣。希望今后有时间能阅读《大学》《中庸》《论语》《孟子》《道德经》《金刚经》《六祖坛经》这几部儒道佛经典著作。&lt;/p&gt;

&lt;h2 id='id1'&gt;哲学&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;哲学，艺术，宗教是人类唯三的精神寄托&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;王子几乎每次都会解释哲学学科的重要性。哲学是解决人内部困难——安顿自己心灵的学科。人与动物的区别在于，人是有意识的。人可以意识到自己的“存在”，并思考存在的意义。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;不仁者，不可以久处约，不可以长处乐。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;不仁：无仁之常体
约：贫
乐：富
心中没有“仁”常驻的人，既不能安于贫困的生活，也不能在富裕中乐业。&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;无法安顿自己心灵的人会更多地遭受外界困难的侵扰。我们学的应用学科是“术”，是工作方法以帮助我们趋利避害。然后在科学中不等于在思想中，在职适中不等于在智慧中。有术无道，无可救药！&lt;/p&gt;

&lt;h2 id='id2'&gt;思想该向何处追溯&lt;/h2&gt;

&lt;p&gt;思想从来都是旧的，新的顶多算是“思潮”。人类思想发展的成果在公元5世纪的“轴心时代”已达顶峰。四大文明古希腊、古中国、古印度、古以色列分别发展出了各自不同的文化成果。以后至今的历史都是一个发展遇到困难问题-〉像“轴心时代”的思想求援，整合转化获得新发展的过程。&lt;/p&gt;

&lt;p&gt;我国上一届以胡锦涛总书记为核心的领导人提出了构建核心价值体系的概念是及时的。我国正处在精神文化的真空状态，社会上充斥着功利主义的话语（无非是集体功利主义与个人功利主义的区别）急需我们探索能够代表我们核心文化价值的表达。&lt;/p&gt;

&lt;p&gt;老师我们的核心文化价值该向属于我国传统文化的儒、道、佛寻求帮助。&lt;/p&gt;

&lt;p&gt;尽管我们社会快速现代化的过程借鉴的是西方发达国家的经验，但西方价值并不普适。印在美国钞票上的“In god and liberty, we trust.”表达了god和liberty是美国社会文化的核心。美国文化宣扬个人主义在实现个人梦想个人价值的意义。在国家危难时，每个公民又变成了爱国主义者，这是在作为新教国家的美国宗教力量的体现。&lt;/p&gt;

&lt;p&gt;然而宗教和“自由”在我国都是不适用的。首先，我国传统思想是无“神”的，佛教中的“佛”强调“佛性即人性”。我国信仰西方宗教的教徒往往也较原宗教的思想是走样的。例如，国外的基督教徒祷告时往往心怀忏悔和感恩，而一个走样的中国教徒可能心里在说“我如此忠心供奉主，求主保佑我儿子高考顺利。”我国对“自由”的理解也与liberty的含义有所不同。我国对自由浅显的理解往往是“自身欲望的满足不受限制”，而liberty真正的含义则是理性之上限制自己的能力（例如，老师在香烟面前是没有自由的）。&lt;/p&gt;

&lt;h2 id='id3'&gt;你的母语是汉语，你就在儒道佛中了&lt;/h2&gt;

&lt;p&gt;语言是我们传递和表达思想的媒介，是我们精神的家。在语言中，我们表达了对“存在”的理解。在上帝创造人类的故事中，上帝先创造了“泥人”，“泥人”与我们的山川河流本质上没有差别的，是上帝吹的一口气，赋予人语言的能力，“泥人”才变成区别于万物的人类。我们的传统思想已经渗透在汉语的方方面面中了。&lt;/p&gt;

&lt;p&gt;一个有利的证据是，许多中文的词汇在英文中是找不到对应的翻译的，这些词语中渗透着传统文化的精髓（接下来三种文化的概念举例也都是这种词汇）。例如，五行金木水火土的翻译，食物寒温热性的翻译，“缘分”一词（中国人的缘分表达了一种偶然+必然的微妙关系）。因此，从考虑传统文化中的核心词汇的翻译入手，有助于思考中外思想的异同之处。&lt;/p&gt;

&lt;h2 id='id4'&gt;儒&lt;/h2&gt;

&lt;p&gt;儒家的核心概念是&lt;strong&gt;“仁”&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;“仁”的思想是我国社会稳定的重要原因。经济学有个基尼系数（Gini coefficient)的概念，是用来衡量一个国家居民内部收入分配差异状况的指标。经济学家认为，当一个国家的基尼系数超过0.4时，这个国家就离社会动乱不远了。我国的基尼系数早在2000年就超过这条警戒线，目前已达0.6以上。在贫富差距较大的社会背景下，我国维系着相对稳定的社会环境，西方经济学家感到诧异。殊不知，这是中国文化使然。中国的老百姓，尤其是传统农民群体信“命”，不怨贫，不仇富，仇的是“为富不仁”。&lt;/p&gt;

&lt;p&gt;让我们来思考一下如何将仁这个概念翻译成英文。kind, honest, love都不合适。在较为接近的love上加一个修饰词变为universal love是一种常见的翻译。然而universal love的博爱也与我们的仁有所区别。博爱是基督教的概念，教育人们众生平等。如果思考我们的双亲和陌生人同时遇难，我们采取什么顺序营救？面对这个问题，universal love强调的平等对应的是无私的爱，而我们的仁爱中包含的“孝”的意味将驱使我们先救双亲。孟子曰：老吾老，以及人之老。我们的仁强调推己及人。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;仁是推己及人的。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;这里的推客分为正推和反推。正推强调“忠”，反推强调“恕”。他们与英文中任何一个有忠诚，饶恕，宽容的词语都有所区别。分别用儒家经典中的一句话来形容：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;正推：己欲利而利人，己欲达而达人。&lt;/p&gt;

&lt;p&gt;反推：己所不欲，勿施于人。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id='id5'&gt;道&lt;/h2&gt;

&lt;p&gt;当今道在英文中的翻译为&lt;em&gt;Taoism&lt;/em&gt;,翻译工作者采用音译的决定也代表着对彻底不可译的妥协。&lt;/p&gt;

&lt;p&gt;道家的核心概念是&lt;strong&gt;“无为”&lt;/strong&gt;。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;无为而无不为。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;对这句话的理解容易陷入误区，以为无所作为是以无所不为为目的。道家的“无为”并不是不作事，强调的是反对做事造作。&lt;/p&gt;

&lt;p&gt;无为的概念尤其强调在道家对统治者的构想上。运用在企业管理中会带来一定启发性。理解“无为”的高明的企业管理者应使企业可以基本脱离自己顺利运作，而不是用强硬的手段严苛的标准。（从历史上看，这种风格的管理实际操作难度较高，效果不如儒家）&lt;/p&gt;

&lt;h2 id='id6'&gt;佛&lt;/h2&gt;

&lt;p&gt;禅宗是佛教的中国化。以下讨论的都为佛教传入中国后融入传统文化的禅宗思想。&lt;/p&gt;

&lt;p&gt;相对于儒家和道家，禅宗的思想在我们的语言留下过更多印记。“真理”“心心相印”“觉悟”都是禅宗思想中的概念。&lt;/p&gt;

&lt;p&gt;（这里老師插入一段关于竞选杨浦区人大代表的轶事。第一个议案是取消双语幼儿园。原因是老師之前教授美国ABC的经验让其认识到牙牙学语时期的双语让儿童失去了一个踏实的精神之家。后来因为非“无知少女”的身份落选。下一次老师还会回來的……）&lt;/p&gt;

&lt;p&gt;“觉悟”是禅宗的一个重要而较难理解的概念。老师曾试图用半个小时向英国的哲学教授解释这个概念，却以失败告终。他解释的过程是这样的：&lt;/p&gt;

&lt;h6 id='__'&gt;首先，悟 不等于 知。&lt;/h6&gt;

&lt;p&gt;知（认知活动）分为能知与所知，无论是能知和所知都是以有所得为目的的，而悟是无所得的。&lt;/p&gt;

&lt;h6 id='id7'&gt;悟是“如桶底子脱”&amp;#8212;禅宗祖师&lt;/h6&gt;

&lt;p&gt;这是对“觉悟”过程的形象比喻。“悟”的那一刻好比：我们的烦恼、疑惑、忧虑、问题如桶中的水，顿悟时的感觉就好像桶底一下子脱掉，所有的问题都不是问题了。&lt;/p&gt;

&lt;p&gt;禅宗祖师的角色类似于欧洲的忏悔神父。当我们有不得解脱的困扰时去拜访禅宗祖师。禅宗祖师通常采取三种方式之一来开导：棒喝（打你）/1-3句话/参话头（反问）&lt;/p&gt;

&lt;p&gt;禅宗祖师的特殊技能在于其觉悟后瞬间的“夺境”“夺人”的技能，准确地抓住来人的弱点，背景的关键矛盾，所谓“抓住机缘”。（我们平时处世也要注意夺人夺境的智慧）&lt;/p&gt;

&lt;h6 id='wu_is_having_a_tacit_agreement_with_nothingness'&gt;Wu is having a tacit agreement with nothingness.&lt;/h6&gt;

&lt;p&gt;经过前两番解释，英国的哲学教授愈加困惑，于是曾在外文出版社做过五年翻译工作的老师尝试用下定义的方式解释这个概念。很遗憾，英国人还是没有顿悟。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;悟是与虚无的默契。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;接下来的时间老师详细地讲了六祖慧能觉悟的故事。&lt;/p&gt;

&lt;p&gt;慧能是禅宗第六代传人，祖籍在广州一代，有一天在街上听人诵读金刚经，感到有所觉悟，便安顿老母，踏上了到湖南找弘忍法师取经之路。见到弘忍法师后，一句“人有南北，佛性无南北”在弘忍法师心里留下良好印象。弘忍法师正值晚年挑选衣钵继承人之际，见到慧能十分喜欢，但为保护他的安危，只是安排瘦小的慧能去舂米房工作。几个月后，慧能命令所有弟子作偈句以表达对佛性的理解。首座大弟子神秀在墙上写下“身是菩提树，心如明镜台，时时勤拂拭，勿使惹尘埃。”然而弘忍法师让众弟子多诵读这个句子将“有大利业”，私下却认为其“未见本心”。慧能的“菩提本无树，明镜亦非台，本来无一物，何处惹尘埃”让弘忍法师看出慧能是唯一已站在“觉悟”门槛上的弟子。弘忍与慧能相约三更为他讲经，在读到“应无所住，而生其心”时，慧能终于了悟了佛性与尘世污秽本就合二为一的道理，彻底觉悟，作出了&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;何期自性本自清淨， 何期自性本不生滅， 何期自性本自具足， 何期自性本無動搖， 何期自性能生萬法。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;弘忍法师见其觉悟，便将衣钵传给了慧能并送他离开寺庙南去，命其隐居15年。&lt;/p&gt;

&lt;p&gt;老师接着讲了弟子们讨伐南去的慧能的故事。弟子们不服气没有出家的慧能继承衣钵便向南追去。最勇武的上座弟子惠明最先追到。慧能准确地把握了惠明爱憎分明的特征（夺境），二句：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;不思善，不思恶。正与么时，哪个是明上座本来面目？&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;惠明顿悟：“如人饮水，冷暖自知。”&lt;/p&gt;

&lt;h6 id='id8'&gt;佛性即人性&lt;/h6&gt;

&lt;p&gt;弘忍之所以将衣钵传给慧能，是因为慧能了悟了“佛性即人性”的道理。这里老师插入了对仓央嘉措《班扎古鲁白玛的沉默》一首诗的赞美。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;你见，或者不见我，我就在那里，不悲不喜。 你念，或者不念我，情就在那里，不来不去。 你爱，或者不爱我，爱就在那里，不增不减。 你跟，或者不跟我，我的手就在你手里，不舍不弃。 来我的怀里，或者，让我住进你的心里。 默然 相爱，寂静 欢喜。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;佛教讲，每个&lt;strong&gt;人注定要爱一次&lt;/strong&gt;。这是佛性的一方面。真正的佛心与人心是合二为一的。&lt;/p&gt;

&lt;p&gt;我们每个人的都有许多表面的标签、身份，能否剥开外在的干扰，看到“本来面目”，看到“本心”，是觉悟的关键。这里老师又举例：想象当今二十一世纪慧能法师还健在，王德峰老师一定去拜访他。正如弘忍见到慧能第一面：“甚麽物？恁麽来？”如果老师回答“复旦大学教授，坐飞机来”明显白瞎……老师的答案将是“不是物，没来过”。&lt;/p&gt;

&lt;h2 id='id9'&gt;总结&lt;/h2&gt;

&lt;p&gt;儒得心，道得自在，佛得解脱。 儒是粮食，道是烹调艺术，佛是药。 儒教我们拿得起，道教我们放得下，佛教我们想得开。&lt;/p&gt;

&lt;h2 id='id10'&gt;提问环节&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;觉悟为何不能翻译为understanding？&lt;/strong&gt; understanding是康德哲学的概念，佛家强调的是生命情感的升华，是生而有之，只是隐藏在表象下的。悟是见本心的过程，与understanding有别。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;如何理解“缘起性空”？&lt;/strong&gt;佛家认为一切皆因缘。讲台上的塑料瓶是因为其背后的来源、生产、销售一系列因缘才成为讲台上的水瓶。当这些缘由消失，这个水平便不是水瓶了。许多道理我们的同学目前还没有足够的人生阅历去领悟。老师年轻时也明白这句话的逻辑，但真正领悟这句话的时候是在父母都过世时。我们的家庭是缘起性空的最好例子。所谓子欲养而亲不在的痛苦，只有经历过的人懂得。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;推荐书目？&lt;/strong&gt;儒：四书（《大学》《中庸》《论语》《孟子》 道：《道德经》 佛：《金刚经》《六祖坛经》&lt;/p&gt;</description>
                <link>http://xcc0322.github.io/University/2014/04/23/wonderfullecture</link>
                <guid>http://xcc0322.github.io/University/2014/04/23/wonderfullecture</guid>
                <pubDate>2014-04-23T00:00:00+08:00</pubDate>
        </item>

        <item>
                <title>WeiboSpammer-Chapter2 WeiboCrawler using Scrapy</title>
                <description>&lt;p&gt;After a bite of Scrapy, we are going to make it work with our target sites, weibo in this case.&lt;/p&gt;

&lt;p&gt;According to the need of my project, I design a crawler with following jobs:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Scratch basic info(numbers of followings and fans, quality of most recent posts) as a signal judging the possibility of anomaly.&lt;/p&gt;
&lt;/li&gt;

&lt;li&gt;
&lt;p&gt;Move to the following and fans pages to crawl the uids of the &lt;em&gt;followed users(followings)&lt;/em&gt; and &lt;em&gt;fans&lt;/em&gt;. Store the connections to build the link graph.&lt;/p&gt;
&lt;/li&gt;

&lt;li&gt;
&lt;p&gt;Crawl and add the profile pages to request list of the users we met in the second step. Repeat step1 and step2.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id='log_in'&gt;Log in&lt;/h4&gt;

&lt;p&gt;Tricky is to log in and acquire the first page. I referred to the work &lt;a href='http://qinxuye.me/article/simulate-weibo-login-in-python/'&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The machinism is a little complicated. We would have to observe the rules little by little with Firebug-like tools. Oberserve parameters in our posts and the content returned by weibo server.&lt;/p&gt;

&lt;p&gt;After following the steps by the link above, I finally make it arrive at the first user profile page of mine. Making sure that this works, I start to build the spider.&lt;/p&gt;

&lt;h4 id='customize_scrapy'&gt;Customize Scrapy&lt;/h4&gt;

&lt;p&gt;The main tasks here are to make our own &lt;strong&gt;Spider, Item and ItemPipeline&lt;/strong&gt; classes, as well as to modify some settings.&lt;/p&gt;

&lt;h6 id='spider'&gt;Spider&lt;/h6&gt;

&lt;p&gt;In Spider class, I build the log-in, parsing and extracting data, and crawling further links procedures. Make every procedure a seperate class and assembly them in WeiboSpider class.&lt;/p&gt;

&lt;p&gt;The concepts are simple. Here are some key points.&lt;/p&gt;

&lt;p&gt;Remember the graph in Chapter 1. The output of Spider class should be Requests and Items to be dispatched to Downloader and ItemPipeline respectively. We either set some &lt;em&gt;start_urls&lt;/em&gt; list or define the &lt;em&gt;start_request&lt;/em&gt; function to generate the first pages. And make every function deal with the response instance and return a list of Items and Requests like this one:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;return [
    Request(
        following_url,
        headers = self.headers,
        callback = self.parse_following_page
    ),Request(
        fans_url,
        headers = self.headers,
        callback = self.parse_fans_page
    ), item
]&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Scrapy use an asynchronization machenism to increase the effiency. Return the Request instance with call_back function and the Scheduler would schedule automatically.&lt;/p&gt;

&lt;p&gt;Extracting links and data with certain pattern in HTML requires using XPath of Selector class.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;from scrapy.selector import Selector

self.sel = Selector(response)

# extract all user ids on the page
uid_list = self.sel.xpath(&amp;#39;//a/@href&amp;#39;).re(r&amp;#39;.*uid=(\d+).*&amp;#39;)

# extract urls of friends&amp;#39; profile pages
urls = self.sel.xpath(&amp;#39;//table/tr/td[2]/a[1]/@href&amp;#39;).extract()

# extract next page url (下页 in Chinese)
next_page = self.sel.xpath(u&amp;#39;//a[text()=\&amp;#39;下页\&amp;#39;]/@href&amp;#39;).extract()

# extract number of commnets (评论 in Chinese)
comments = self.sel.xpath(&amp;#39;//a/text()&amp;#39;).re(ur&amp;#39;^评论\[(\d+)\]&amp;#39;)&lt;/code&gt;&lt;/pre&gt;

&lt;h6 id='run'&gt;Run&lt;/h6&gt;

&lt;p&gt;The Item class definition requires almost no tricks. After building the Spider and Item class as well as customize the settings file, we have made a working crawler which could crawl and extract the target item as wanted.&lt;/p&gt;

&lt;p&gt;Turn on the logging in Spider and run the crawler, we will see the details in LOG_FILE&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;log.start(logfile=&amp;#39;LOG_FILE&amp;#39;)
&amp;gt;scrapy crawl weibo&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We could direct the crawler to output items in a json format to local file.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;scrapy crawl weibo -o items.json -t json&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Congratulations! If the .json file fulfills your data requirement, then your work has finished.&lt;/p&gt;

&lt;h6 id='itempipeline'&gt;ItemPipeline&lt;/h6&gt;

&lt;p&gt;Itempipeline is for filters and further export operations on result Items. Complicated export operations may also be defined in Item Exporters.&lt;/p&gt;

&lt;p&gt;I stored my data in MySql database, so I make a connection with my db using twisted api and deal with tables here. Before the crawling, set up the database and create the tables.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;from twisted.enterprise import adbapi

self.dbpool = adbapi.ConnectionPool(&amp;#39;MySQLdb&amp;#39;, **dbargs)
d = self.dbpool.runInteraction(self._do_upsert, item, spider)

def _do_upsert(self, conn, item, spider):&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Refer dirbot educational example for further details.&lt;/p&gt;

&lt;p&gt;Now I have completed my crawler and store the results in Mysql database:)&lt;/p&gt;

&lt;h4 id='problems__solutions'&gt;Problems &amp;amp; Solutions&lt;/h4&gt;

&lt;h6 id='request_priority'&gt;Request Priority&lt;/h6&gt;

&lt;p&gt;In Spider class, my extracted links include the &lt;strong&gt;friends profile pages&lt;/strong&gt; and &lt;strong&gt;next pages&lt;/strong&gt;. The next pages are due to the limit of total friends displaying in one page. I want my crawler to follow a bread-first method. Here the priority feature of Request Objects solves my problem. Just set &lt;strong&gt;next pages&lt;/strong&gt; a higher priority than &lt;strong&gt;friends profile pages&lt;/strong&gt;!&lt;/p&gt;

&lt;h6 id='pause_and_resumefailed'&gt;Pause and Resume(failed)&lt;/h6&gt;

&lt;p&gt;Why I need to pause and resume the crawer?&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;I want a larger dataset, as this is a link analysis on a social graph. My laptop usage are not sure to be insistant and our network is unstable.&lt;/p&gt;
&lt;/li&gt;

&lt;li&gt;
&lt;p&gt;weibo has some prohibition that causes a frequent stop.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The official document provide &lt;a href='http://doc.scrapy.org/en/latest/topics/jobs.html?highlight=pause'&gt;a way&lt;/a&gt; to do this by direct a JOBDIR to store the status of unfinished Requests and Request Filters. This requires you to keep serializable characteristic.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;scrapy crawl weibo -s JOBDIR=cache/persistence&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;However, I failed to work with it. Maybe I have some problems with the serializable requirement. As I later found the data volume is passable for my project, I gave up this feature.&lt;/p&gt;

&lt;h4 id='bug_shooting__tips'&gt;Bug Shooting &amp;amp; Tips&lt;/h4&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Helpful references:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href='http://deerchao.net/tutorials/regex/regex.htm'&gt;regex&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href='http://www.w3schools.com/XPath/'&gt;XPath&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;

&lt;li&gt;
&lt;p&gt;If you want to use Mysql, install mysql_python with easy_install first.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;</description>
                <link>http://xcc0322.github.io/cs/2014/04/20/weibospammer2</link>
                <guid>http://xcc0322.github.io/cs/2014/04/20/weibospammer2</guid>
                <pubDate>2014-04-20T00:00:00+08:00</pubDate>
        </item>

        <item>
                <title>WeiboSpammer-Chapter1 Scrapy</title>
                <description>&lt;p&gt;&lt;img alt='Scarpy' src='http://doc.scrapy.org/en/latest/_images/scrapy_architecture.png' style='display: block;margin-left: auto;margin-right:auto;width:100%' /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href='http://scrapy.org/'&gt;Scrapy&lt;/a&gt; is a popular crawler framework in Python. Recently I used it to build a weibo crawler to crawl some data for my WeiboSpammer project on Data Mining course.&lt;/p&gt;

&lt;p&gt;I install the framework with Windows8 and Python2.7. Here are the details:&lt;/p&gt;

&lt;h4 id='1read_scrapy_at_a_glance_to_gain_an_overview'&gt;1.Read &lt;a href='http://doc.scrapy.org/en/latest/intro/overview.html'&gt;Scrapy at a glance&lt;/a&gt; to gain an overview.&lt;/h4&gt;

&lt;h4 id='2install_scrapy'&gt;2.Install Scrapy.&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Install OpenSSL(install VC++2008 redistributables first)&lt;/li&gt;

&lt;li&gt;Install pip&lt;/li&gt;

&lt;li&gt;Install lxml(I fail to do this with pip but make it with easy_install)&lt;/li&gt;

&lt;li&gt;Install Scrapy&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Then I start playing with the framework.&lt;/p&gt;

&lt;h4 id='3start_with_the_tutorial'&gt;3.Start with the tutorial&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;scrapy startproject tutorial&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The command line tells me zope.install in need!&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;easy_install zope.install&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There appears another error. Luckily someone else met it before and asked on &lt;a href='http://stackoverflow.com/questions/22800768/getting-error-dll-load-failed-the-operating-system-cannot-run-1-python-2-7'&gt;stackflow&lt;/a&gt;. I copied the ssleay.dll and libeay32.dll from openssl directory to system32. Restart. Tutorial project built!&lt;/p&gt;

&lt;p&gt;I followed the instructions to write the classes. However I encountered another problem &amp;#8220;No module named win32api&amp;#8221;. The tutorial said: You need to install pywin32 because of this Twisted bug. So I installed pywin32 from &lt;a href='http://sourceforge.net/projects/pywin32/files/pywin32/Build%20218/'&gt;here&lt;/a&gt;. It works! The first crawler crawls two sites successfully.&lt;/p&gt;

&lt;h4 id='4customize'&gt;4.Customize&lt;/h4&gt;

&lt;p&gt;After this we have gain a basic overview of Scarpy. Now we can turn to our own project and learn further skills while solving our problems.&lt;/p&gt;</description>
                <link>http://xcc0322.github.io/cs/2014/04/20/weibospammer1</link>
                <guid>http://xcc0322.github.io/cs/2014/04/20/weibospammer1</guid>
                <pubDate>2014-04-20T00:00:00+08:00</pubDate>
        </item>

        <item>
                <title>WeiboSpammer-Chapter0 Introduction</title>
                <description>&lt;p&gt;&lt;img alt='weibo_logo' src='http://blogs-images.forbes.com/rebeccafannin/files/2014/04/5812606276_2401e9c0f9_b.jpg' style='display: block;margin-left: auto;margin-right:auto;width:100%' /&gt;&lt;/p&gt;

&lt;p&gt;This is the project for my data mining course this semester.&lt;/p&gt;

&lt;p&gt;The course is based on &lt;a href='http://infolab.stanford.edu/~ullman/mmds/book.pdf'&gt;this textbook&lt;/a&gt; by Stanford University. The contents are well strutured and very pratical.&lt;/p&gt;

&lt;h4 id='the_spammer_problem_description'&gt;The Spammer Problem Description&lt;/h4&gt;

&lt;p&gt;Recently there&amp;#8217;s a spammer problem with weibo. Some tricky guys make spammer farms and sell the fans number increasement for those who want to increase their popularity in a short time. These spammers are not like the traditional zombie accounts. They have normal head portraits and make interaction actively with true users. Some of them follow each other. Some follow users randomly. The master guy sell it online like in taobao.com.&lt;/p&gt;

&lt;p&gt;I observed this because every day I gained some zombie fans which is kind of annoying. One of my roommates start a shop online recently and use the spammers to do a propagation.&lt;/p&gt;

&lt;p&gt;After the lecture of link analysis, I thought the algorithm on spam detection might work with this spammer problem.&lt;/p&gt;

&lt;h4 id='the_algorithm'&gt;The Algorithm&lt;/h4&gt;

&lt;p&gt;I proposed the project to my teacher and planned to deploy the treatment with link spam. The teacher suggested two further directions and gave references to two papers. &lt;a href='http://www.cs.cmu.edu/~mmcgloho/pubs/snare.pdf'&gt;SNARE&lt;/a&gt; and &lt;a href='http://www.cs.ust.hk/~qyang/Docs/2012/AAAI_2012_Spammers.pdf'&gt;SMFSR&lt;/a&gt;. They are about link analysis on anomaly detection and matrix factorization respectively. After reading them, I found the first one fits my problem better and decided to implement it.&lt;/p&gt;

&lt;h4 id='the_data'&gt;The Data&lt;/h4&gt;

&lt;p&gt;The first step is to get my data. I mainly need two part of data:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;links&lt;/strong&gt; the fans and followed weiboers pairs&lt;/p&gt;
&lt;/li&gt;

&lt;li&gt;
&lt;p&gt;&lt;strong&gt;user profile&lt;/strong&gt; the profile to decide a starting value of anomaly&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It&amp;#8217;s hard to find fit dataset. Immediately I turn to crawling.&lt;/p&gt;

&lt;p&gt;My friends suggested crawling with weibo API. I had a try but later found the frequency and user authentication are too limited for my need. So we have to extract what we want from html pages directly. I played with Scrapy and found it worked.&lt;/p&gt;

&lt;h4 id='the_schedule'&gt;The Schedule&lt;/h4&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;15 propose and start&lt;/p&gt;
&lt;/li&gt;

&lt;li&gt;
&lt;p&gt;20 data crawling finised&lt;/p&gt;
&lt;/li&gt;

&lt;li&gt;
&lt;p&gt;21-5.15 algorithm implementation&lt;/p&gt;
&lt;/li&gt;

&lt;li&gt;
&lt;p&gt;15-5.30 result anaylisis and report&lt;/p&gt;
&lt;/li&gt;

&lt;li&gt;
&lt;p&gt;30 deadline&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id='about_the_posts'&gt;About the posts&lt;/h4&gt;

&lt;p&gt;This is my first technical post on this blog. I wrote in a mixed style of recording and tutorial. The main target is to help myself understand the process better. I&amp;#8217;ll be happy if it helps readers in a way of bug shooting or inspiration.&lt;/p&gt;</description>
                <link>http://xcc0322.github.io/cs/2014/04/20/weibospammer0</link>
                <guid>http://xcc0322.github.io/cs/2014/04/20/weibospammer0</guid>
                <pubDate>2014-04-20T00:00:00+08:00</pubDate>
        </item>

        <item>
                <title>Hello World</title>
                <description>&lt;p&gt;Test with jekyll-bootstrap!&lt;/p&gt;</description>
                <link>http://xcc0322.github.io/snippet/2014/04/15/hello-world</link>
                <guid>http://xcc0322.github.io/snippet/2014/04/15/hello-world</guid>
                <pubDate>2014-04-15T00:00:00+08:00</pubDate>
        </item>

        <item>
                <title>序言</title>
                <description>&lt;p&gt;&lt;img alt='photo' src='http://tmhome.com/wp-content/uploads/2013/08/tumblr_m6c3ipVPWl1romywno1_500.gif' style='display: block;margin-left: auto;margin-right:auto;width:100%' /&gt;&lt;/p&gt;

&lt;p&gt;现在是2014年1月11日。我刚刚考完大三上学期的最后一门考试，在去哈尔滨玩耍之前的两天空档期。 我终于有机会做这件积攒已久的task——再开blog。写blog是一件十分有趣且有意义的事，尤其对我这个特别念旧又特别爱学习的人。无论是技术blog，游记，记录生活鸡毛蒜皮的流水账，与人共享、日后回味都有助于自己的学习知识、反思教训和体验生活。&lt;/p&gt;

&lt;p&gt;这件事一直拖延至今，原因主要有下：平日课程繁忙时间有限，自己的兴趣点过于分散，微博和人人用的太多，一直没选到理想的永久的blog host，加之开blog写字本身对于我的难度而导致的拖延心理。&lt;/p&gt;

&lt;p&gt;简而言之，懒。可是再不行动起来，就要毕业了。&lt;/p&gt;

&lt;p&gt;希望在抽空把大学以来丢失的记录补一补，包括：1.一些重要的事情 2.一些情绪激动的瞬间 3.一些值得积累的技术笔记 4.一些杂文&lt;/p&gt;

&lt;p&gt;另外，争取把自己拙劣的pj收集一下，放在这里，积累起来。&lt;/p&gt;

&lt;h2 id='blog'&gt;在这些没写blog的日子里&lt;/h2&gt;

&lt;p&gt;截至目前，大学的轨迹主要分散在人人、微博、经常断篇又重来的日记本和我把照片洗成卡片状做成的diy相册里。另有各种技术的小笔记在手写的本子上和网易云笔记里。&lt;/p&gt;

&lt;p&gt;印象中，最近一次还保持认真整理照片、记录人生的好习惯是在2011年高中毕业的时候，有些久远了。其实这两年来，以我为主角，还是发生过不少故事的。&lt;/p&gt;

&lt;p&gt;2011年我放弃高考，被保送到了复旦大学念CS系，9月份离开了我生长了18年的家乡青岛，来到了魔都上海。大一的我，生活在复旦最美的邯郸老校区，过了一年的“书院制生活”。由于旦旦计算机较为边缘的尴尬地位，教学安排十分松散很不科学，加上通识教育的特殊政策，大一的课业较为轻松，以数学物理基础课为主。回忆起来，这是我第一次挣脱了义务教育的枷锁，走入了我的大学。课余时间我既没有刻苦学习，也没有疯狂地玩耍。我读各种书，听讲座，参加社团活动，翘课颇少，认真听讲，做论文做pre。没有超额用力地生活，只是安静的享受了每一分每一秒，不过五味杂尘也回味良多啦。尽管没有碰到一个心爱的男盆友与我一起共度这段时光有些遗憾，但不得不说，这一年的生活是相当丰富多彩的，作为我18岁的回忆，我很满足。&lt;/p&gt;

&lt;p&gt;从这以后，我的生活能力和思维能力开始变得独立。复旦的校风是十分优秀和独特的。名曰“自由而无用的灵魂”的一种风格在我心里埋下了种子。从初来乍到的各种看不惯，到渐渐地爱上这里的过程中，我开始理解自由，开始思考，开始有了把握人生的能力。到现在，这股精神已经成为我的好伙伴，我清楚地认为我对这股灵魂的力量的崇尚是理性的，我想她还会继续陪我走很长一段路。&lt;/p&gt;

&lt;p&gt;大二，我来到了张江。张江的生活非常符合我想象中西南某高校的风格，可惜人更少，专业更单一，心里有些悲伤。一开学，顺势报名了下半学期的台湾的交换生。来到张江索性入乡随俗，大二上戒了我随性参加活动的风格（但我还是义务地第二次参加了一次妖而久和一些院系活动）出没图书馆，努力刷绩点。十分怀念本部。但我的心里留过自由的火种，这颗灵魂还是活下来并适应了这里的生活，这样一种环境创造了许多独处的机会，甚至让心智更成熟了些。&lt;/p&gt;

&lt;p&gt;大二下学期，我在台湾的国立交通大学做交换生。从2月19日抵达台北到6月28日返回上海的&lt;strong&gt;每一天每一小时&lt;/strong&gt;都是精彩纷呈的。我认识了一群新朋友，开心地环游了台湾。这是一次&lt;strong&gt;真正的&lt;/strong&gt;旅行。我爱上了这片土地，爱上了旅行的感觉。人在异域，接受了社会环境文化教育冲突，我在这片热情友好甚至更加自由的土地上得到了新的进步。此外，大陆与台湾藕断丝连的关系和特殊的政治背景也让我对许多问题有了一些新的认识。&lt;/p&gt;

&lt;p&gt;2013年的我异常好运，难忘的台湾旅行后是难忘的暑期实习。在台期间我通过电话参加并通过了谷歌上海办公室暑假实习的面试。2013年7月1日到10月25日，我非常幸运地进入了这家优秀的公司做实习。太爱这段时光！我的team既高效又欢乐的team，成员特别乐于自黑与互黑，工程师们互相爱护，互相帮助~ 我的host经验丰富，既友善又靠谱，遇bug淡定，人超nice。总之公司的实习项目设计合理，让我学到了很多。太多意料之外的技术与非技术的经历让我度过了一个完美的暑假。最大的收获就是：我开始对自己在专业上的兴趣有信心，开始对自己的能力有信心。这对我接下来的专业学习产生了巨大的作用！&lt;/p&gt;

&lt;p&gt;大三回到张江，我学会更理智地看待生活。生活开始变得有效率。作业，pj，复习，考试以及各种活动和面试都按部就班地行进。课余时间还安排了到苏州和南京的两次旅行，计划已久的1月份哈尔滨旅行马上就可以完成自己积攒多年的冬天去东北的心愿了！之后还可以借着冬令营的机会去香港玩一下！用上海普通话的味道讲就是“不要太爽！”&lt;/p&gt;

&lt;p&gt;时间过得很快，大学生活已过去了5/8，而最理想、最疯狂和无忧无虑的部分已过去。最近我也在考虑毕业计划的事情了。入学时拟定的出国的计划被实习的过程打断了一下。希望2月份到来之前我可以做一个决定。&lt;/p&gt;

&lt;h2 id='2014'&gt;2014&lt;/h2&gt;

&lt;p&gt;希望我这次blog持续的久一点，把那些我爱的人和我爱的故事记下来。很久没写字，文字变生疏了好多，这篇blog也破例在发布之后多次修改。请见谅。&lt;/p&gt;</description>
                <link>http://xcc0322.github.io/blog/2014/01/11/prologue</link>
                <guid>http://xcc0322.github.io/blog/2014/01/11/prologue</guid>
                <pubDate>2014-01-11T14:08:31+08:00</pubDate>
        </item>


</channel>
</rss>
